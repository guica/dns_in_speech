{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b10035-d248-44c0-80a4-bfa377ab4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/tf/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4659a7c-0fdc-4cb0-b205-64531953dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import NoisyTargetGenerator\n",
    "from sound import Sound\n",
    "import numpy as np\n",
    "from artigos.Transformer import getTransformerLayers\n",
    "from utils import calculate_stft_magnitude_and_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "135e8f95-7423-44a0-a92e-50c1b12b16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Reshape, Input\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe435ced-3b86-46d6-b566-12de26551e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, num_positions, d_model, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_positions = num_positions\n",
    "        self.pos_encoding = self.positional_encoding(num_positions, d_model)\n",
    "\n",
    "    def positional_encoding(self, num_positions, d_model):\n",
    "        pos_enc = np.zeros((num_positions, d_model))\n",
    "        for pos in range(num_positions):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        return tf.cast(pos_enc, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:tf.shape(inputs)[1]]\n",
    "\n",
    "def image_to_patches_with_positional_encoding(image, segment_size, num_channels):\n",
    "    # Crie um layer para dividir a imagem em segmentos/palavras\n",
    "    segment_layer = Conv2D(filters=num_channels * segment_size * segment_size, kernel_size=(segment_size, segment_size), strides=(segment_size, segment_size), padding='valid')\n",
    "\n",
    "    # Converta a imagem em segmentos\n",
    "    patches = segment_layer(image)\n",
    "    d_model = segment_size * segment_size * num_channels  # Dimensão do embedding\n",
    "\n",
    "    # Redimensione os patches para a forma correta para o Transformer\n",
    "    patches = Reshape((-1, d_model))(patches)  # 'batch_size' é substituído por '-1' para flexibilidade\n",
    "\n",
    "    # Número total de posições (segmentos)\n",
    "    num_positions = (original_height // segment_size) * (original_width // segment_size)\n",
    "\n",
    "    # Aplique o embedding posicional\n",
    "    positional_encoding_layer = PositionalEncoding(num_positions, d_model)\n",
    "    encoded_patches = positional_encoding_layer(patches)\n",
    "\n",
    "    return encoded_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfa3df05-0840-417e-b735-f5baa556eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageReconstructionLayer(Layer):\n",
    "    def __init__(self, original_height, original_width, segment_size, num_channels, **kwargs):\n",
    "        super(ImageReconstructionLayer, self).__init__(**kwargs)\n",
    "        self.original_height = original_height\n",
    "        self.original_width = original_width\n",
    "        self.segment_size = segment_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def call(self, encoded_patches):\n",
    "        num_segments_height = self.original_height // self.segment_size\n",
    "        num_segments_width = self.original_width // self.segment_size\n",
    "\n",
    "        # Redimensione os patches para a forma original de segmentos\n",
    "        reshaped_patches = tf.reshape(encoded_patches, (-1, num_segments_height, num_segments_width, self.segment_size, self.segment_size, self.num_channels))\n",
    "\n",
    "        # Reorganize os patches para a forma de uma imagem\n",
    "        batch_size = tf.shape(reshaped_patches)[0]\n",
    "        reshaped_image = tf.reshape(reshaped_patches, (batch_size, self.original_height, self.original_width, self.num_channels))\n",
    "\n",
    "        # Redimensione a largura para 1 enquanto mantém a altura e os canais\n",
    "        resized_image = tf.image.resize(reshaped_image, [self.original_height, 1])\n",
    "\n",
    "        return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3e5ba5d-792e-4e5d-b054-3bcac2a136c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGenerator(NoisyTargetGenerator):\n",
    "    def __init__(self, sound_files, noise_files, block_size=2, normalize_phi=True):\n",
    "        super().__init__(sound_files, noise_files, block_size=block_size, normalize_phi=normalize_phi)\n",
    "\n",
    "    def generate_batch_transformer(self, clean_phasors):\n",
    "        \"\"\"\n",
    "        Generate a batch of transformed arrays and y_samples from clean and noisy phasors.\n",
    "    \n",
    "        :param clean_phasors: Numpy array of shape (128, 64, 2), representing the clean phasors.\n",
    "        :return: Tuple containing a list of 64 transformed arrays of shape (128, 128, 2) and an array y_samples of shape (64, 128, 1, 2).\n",
    "        \"\"\"\n",
    "    \n",
    "        # Criando um novo array com shape (128, 64, 2) com zeros\n",
    "        new_array = np.zeros((128, 64, 2))\n",
    "    \n",
    "        # Preenchendo o novo array com os valores do array clean_phasors\n",
    "        # new_array[:, :64, :] = clean_phasors\n",
    "    \n",
    "        # Inicializando a lista para armazenar os 64 arrays\n",
    "        arrays = []\n",
    "    \n",
    "        # Inicializando o array y_samples com o shape desejado (64, 128, 1, 2)\n",
    "        y_samples = np.zeros((64, 128, 1, 2))\n",
    "    \n",
    "        for i in range(64):\n",
    "            # Copiando o array new_array para um novo array temporário\n",
    "            temp_array = np.copy(new_array)\n",
    "            \n",
    "            # Substituindo os valores em temp_array pelos valores correspondentes de clean_phasors\n",
    "            if i != 0:\n",
    "                temp_array[:, :i, :] = clean_phasors[:, :i, :]\n",
    "    \n",
    "            # Adicionando o temp_array à lista\n",
    "            arrays.append(temp_array)\n",
    "    \n",
    "            # Preenchendo y_samples com os valores correspondentes de clean_phasors\n",
    "            y_samples[i, :, 0, :] = clean_phasors[:, i, :]\n",
    "    \n",
    "        return np.array(arrays), y_samples\n",
    "            \n",
    "        \n",
    "    \n",
    "    def generate_sample_completo(self, batch_size=32, include_clean=False):\n",
    "        while True:\n",
    "            # Carrega um lote de vozes e ruidos\n",
    "            sound_batch, noise_batch = self.pick_random_blocks(batch_size)\n",
    "\n",
    "            xn_train = []\n",
    "            xc_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            # Adiciona ruído a cada som e calcula a nota PESQ\n",
    "            for sound, noise in zip(sound_batch, noise_batch):\n",
    "\n",
    "                sound_escalado, noisy_sound = self.normalize_and_add_noise(sound, noise)\n",
    "                \n",
    "                if sound_escalado is None or noisy_sound is None:\n",
    "                    continue\n",
    "                \n",
    "                # try:\n",
    "                A, phi, _, _ = calculate_stft_magnitude_and_phase(sound_escalado)\n",
    "                A_noisy, phi_noisy, _, _ = calculate_stft_magnitude_and_phase(noisy_sound)\n",
    "                # except:\n",
    "                #     continue\n",
    "\n",
    "                F = self.assemble_phasors(A, phi)\n",
    "                F_noisy = self.assemble_phasors(A_noisy, phi_noisy)\n",
    "\n",
    "                xc_samples, y_samples = self.generate_batch_transformer(F)\n",
    "                \n",
    "                # Adiciona os exemplos aos lotes de treinamento\n",
    "                for _ in range(64):\n",
    "                    xn_train.append(F_noisy)\n",
    "                xc_train.append(xc_samples)\n",
    "                y_train.append(y_samples)\n",
    "                \n",
    "                if include_clean:\n",
    "                    xc_samples, y_samples = self.generate_batch_transformer(F)\n",
    "                    for _ in range(64):\n",
    "                        xn_train.append(F)\n",
    "                    xc_train.append(xc_samples)\n",
    "                    y_train.append(y_samples)\n",
    "\n",
    "            xn_train = np.array(xn_train)\n",
    "            xc_train = np.array(xc_train)\n",
    "            y_train = np.array(y_train)\n",
    "            \n",
    "            yield [xn_train, xc_train.reshape(-1, 128, 64, 2)], y_train.reshape(-1, 128, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006f7fb8-395c-4c67-8376-c4cda7b7f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_shape_size = 8192\n",
    "ws = 255\n",
    "ol = 128\n",
    "input_shape = (128, 64, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152c7572-2666-435c-a394-e71c0bcdf1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Speech Files: 100%|██████████| 8179/8179 [00:05<00:00, 1369.52it/s]\n",
      "Loading Noise Files: 100%|██████████| 8137/8137 [00:06<00:00, 1341.34it/s]\n",
      "/tf/utils/sound.py:65: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  clean_sounds = [sound for sound in clean_sounds if sound != self.TOO_SHORT_ERROR]\n",
      "/tf/utils/sound.py:77: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  noise_sounds = [sound for sound in noise_sounds if sound != self.TOO_SHORT_ERROR]\n"
     ]
    }
   ],
   "source": [
    "sound_base = Sound('/tf/Dados/Vozes/', '/tf/Dados/Ruido/', base_shape_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32804fcc-2ac8-44a4-9abb-9dd25a0a2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_train = TransformerGenerator(sound_base.train_X, sound_base.noise_sounds)\n",
    "data_generator_val = TransformerGenerator(sound_base.val_X, sound_base.noise_sounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9713de3f-520f-4b4a-8645-a36aeb88b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina o tamanho do segmento e o número de canais\n",
    "segment_size = 16\n",
    "num_channels = 2  # Número de canais na imagem\n",
    "\n",
    "# Calcule as dimensões do embedding\n",
    "original_height = 128\n",
    "original_width = 64\n",
    "num_segments_height = original_height // segment_size\n",
    "num_segments_width = original_width // segment_size\n",
    "d_model = segment_size * segment_size * num_channels  # Dimensão do embedding\n",
    "\n",
    "# Ajuste o código do modelo para usar estas funções\n",
    "input_image_encoder = Input(shape=(original_height, original_width, num_channels))\n",
    "input_image_decoder = Input(shape=(original_height, original_width, num_channels))\n",
    "\n",
    "# Processamento de embedding\n",
    "encoded_patches_encoder = image_to_patches_with_positional_encoding(input_image_encoder, segment_size, num_channels)\n",
    "encoded_patches_decoder = image_to_patches_with_positional_encoding(input_image_decoder, segment_size, num_channels)\n",
    "\n",
    "# Passar os embeddings pelas camadas do Transformer\n",
    "transformer_output = getTransformerLayers(encoded_patches_encoder, encoded_patches_decoder, d_model=d_model)\n",
    "\n",
    "# Reconstrução da imagem de saída\n",
    "reconstruction_layer = ImageReconstructionLayer(original_height, original_width, segment_size, num_channels)\n",
    "output_image = reconstruction_layer(transformer_output)\n",
    "output_image = Reshape((original_height, 1, num_channels))(output_image)\n",
    "\n",
    "# Construção do modelo\n",
    "transformer_model = Model(inputs=[input_image_encoder, input_image_decoder], outputs=output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adec92bb-aa09-4c55-9ef8-721be43b177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_44\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_81 (InputLayer)          [(None, 128, 64, 2)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 4, 512)    262656      ['input_81[0][0]']               \n",
      "                                                                                                  \n",
      " input_82 (InputLayer)          [(None, 128, 64, 2)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " reshape_21 (Reshape)           (None, 32, 512)      0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 8, 4, 512)    262656      ['input_82[0][0]']               \n",
      "                                                                                                  \n",
      " positional_encoding_20 (Positi  (None, 32, 512)     0           ['reshape_21[0][0]']             \n",
      " onalEncoding)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_22 (Reshape)           (None, 32, 512)      0           ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " model_40 (Functional)          (None, None, 512)    1577984     ['positional_encoding_20[0][0]'] \n",
      "                                                                                                  \n",
      " positional_encoding_21 (Positi  (None, 32, 512)     0           ['reshape_22[0][0]']             \n",
      " onalEncoding)                                                                                    \n",
      "                                                                                                  \n",
      " model_41 (Functional)          (None, None, 512)    1577984     ['model_40[0][0]']               \n",
      "                                                                                                  \n",
      " model_42 (Functional)          (None, None, 512)    2629632     ['positional_encoding_21[0][0]', \n",
      "                                                                  'model_41[0][0]']               \n",
      "                                                                                                  \n",
      " model_43 (Functional)          (None, None, 512)    2629632     ['model_42[0][0]',               \n",
      "                                                                  'model_41[0][0]']               \n",
      "                                                                                                  \n",
      " dense_362 (Dense)              (None, 32, 512)      262656      ['model_43[0][0]']               \n",
      "                                                                                                  \n",
      " image_reconstruction_layer_5 (  (None, 128, 1, 2)   0           ['dense_362[0][0]']              \n",
      " ImageReconstructionLayer)                                                                        \n",
      "                                                                                                  \n",
      " reshape_23 (Reshape)           (None, 128, 1, 2)    0           ['image_reconstruction_layer_5[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,203,200\n",
      "Trainable params: 9,203,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8020147a-d6da-4bf0-acbd-b07194b394cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='msle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32dbc9d8-9fbd-46cf-ae20-a2b9ccc4a8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1\n",
      " 3936/16114 [======>.......................] - ETA: 20:15 - loss: 0.0830"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m validation_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_generator_val\u001b[38;5;241m.\u001b[39mgenerate_sample_completo(batch_size\u001b[38;5;241m=\u001b[39mbatch_size))\n\u001b[1;32m     11\u001b[0m [x1_val, x2_val], y_val \u001b[38;5;241m=\u001b[39m validation_batch\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_generator_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_sample_completo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_clean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx1_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1690\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1691\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:680\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:673\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 673\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "steps_per_epoch = len(sound_base.train_X) // batch_size\n",
    "\n",
    "print('Starting training')\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    # Gera um novo lote de validação para cada época\n",
    "    validation_batch = next(data_generator_val.generate_sample_completo(batch_size=batch_size))\n",
    "    [x1_val, x2_val], y_val = validation_batch\n",
    "    \n",
    "    transformer_model.fit(data_generator_train.generate_sample_completo(batch_size=batch_size, include_clean=False),\n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     epochs=1,\n",
    "                     validation_data=([x1_val, x2_val], y_val),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a38bcd-6dd3-46de-b41d-f56a2d5d77c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba9b67-edd9-46c5-b594-104c6fb47c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
