{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a0ad97-f8ed-4fc5-acfd-188d592a7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/tf/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8c9e74-5718-46f6-9c6d-d000cf70b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_shape_size = 8192\n",
    "ws = 255\n",
    "ol = 128\n",
    "input_shape = (128, 64, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee14a338-6a30-4ba3-93e6-60bb0fe8f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_stft_magnitude_and_phase, reconstruct_signal_from_stft\n",
    "from sound import Sound\n",
    "from data_generators import NoisyTargetGenerator\n",
    "from artigos.PRIDNet import create_model\n",
    "import tensorflow as tf\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio\n",
    "from IPython import display\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1214ce-f3b9-43fb-a7e8-e301715e95ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Speech Files: 100%|██████████| 8179/8179 [00:05<00:00, 1513.68it/s]\n",
      "Loading Noise Files: 100%|██████████| 8137/8137 [00:05<00:00, 1444.38it/s]\n",
      "/tf/utils/sound.py:65: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  clean_sounds = [sound for sound in clean_sounds if sound != self.TOO_SHORT_ERROR]\n",
      "/tf/utils/sound.py:77: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  noise_sounds = [sound for sound in noise_sounds if sound != self.TOO_SHORT_ERROR]\n"
     ]
    }
   ],
   "source": [
    "sound_base = Sound('../../../Dados/Vozes/', '../../../Dados/Ruido/', base_shape_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41feb384-a619-408d-90a6-faaa3b7edd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_train = NoisyTargetGenerator(sound_base.train_X, sound_base.noise_sounds)\n",
    "data_generator_val = NoisyTargetGenerator(sound_base.val_X, sound_base.noise_sounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebc903b-8bf1-4962-8b46-6a23184d9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    \n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(22, 7))\n",
    "    \n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Log Power Spectrum - Som ruidoso', 'Log Power Spectrum - Som original', 'Log Power Spectrum - Som filtrado']\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(10 * np.log10((display_list[i][..., 0])**2), aspect='auto', cmap='inferno')\n",
    "        plt.colorbar(format='%+2.0f dB')  # Removi a variável 'im' e 'axs[0]'\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4009a6f8-a58c-45b4-87ec-2646a3201434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "#     # TensorFlow operations that you want to run on the GPU\n",
    "\n",
    "#     model = create_model(input_shape)\n",
    "\n",
    "#     print(model.summary())\n",
    "\n",
    "#     steps_per_epoch_train = len(sound_base.train_X)\n",
    "#     steps_per_epoch_validation = len(sound_base.val_X)\n",
    "\n",
    "#     best_models_path = \"/tf/Etapa 3/Artigos/PRIDNet/model_checkpoints\"\n",
    "#     callbacks_lst = [\n",
    "#                      tf.keras.callbacks.ModelCheckpoint(filepath=best_models_path+\"best_PRIDNet_blindnoise_128x64.h5\", save_freq=100, save_weights_only=False),\n",
    "#         tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', min_lr=0.0000009, min_delta=0.0001, factor=0.70, patience=3, verbose=1, mode='min'),\n",
    "#         tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, min_delta=0.0001, patience=10)\n",
    "#     ]\n",
    "\n",
    "#     model.compile(loss=tf.keras.losses.MeanSquaredLogarithmicError(), optimizer=Adam(learning_rate=0.0009))\n",
    "\n",
    "#     batch_size = 32\n",
    "#     steps_per_epoch = len(sound_base.train_X) // batch_size\n",
    "    \n",
    "#     print('Starting training')\n",
    "    \n",
    "#     for i, epoch in enumerate(range(20)):\n",
    "#         print(f\"Epoch {epoch + 1}\")\n",
    "        \n",
    "#         # Gera um novo lote de validação para cada época\n",
    "#         validation_batch = next(data_generator_val.generate_sample_completo(batch_size=batch_size))\n",
    "#         x_val, y_val = validation_batch\n",
    "    \n",
    "#         model.fit(data_generator_train.generate_sample_completo(batch_size=batch_size, include_clean=True),\n",
    "#                   steps_per_epoch=steps_per_epoch,\n",
    "#                   epochs=1,\n",
    "#                   validation_data=(x_val, y_val),\n",
    "#                   callbacks=callbacks_lst\n",
    "#                  )\n",
    "    \n",
    "#         indice_aleatorio = np.random.choice(x_val.shape[0])\n",
    "        \n",
    "#         # Selecione a amostra correspondente\n",
    "#         amostra_noisy_module = x_val[indice_aleatorio]\n",
    "#         amostra_noisy_module = amostra_noisy_module[np.newaxis, ...]\n",
    "    \n",
    "#         amostra_original_module = y_val[indice_aleatorio]\n",
    "#         amostra_original_module = amostra_original_module[np.newaxis, ...]\n",
    "        \n",
    "#         generate_images(model, amostra_noisy_module, amostra_original_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b439de-0205-4660-aff4-94fd5b66a32a",
   "metadata": {},
   "source": [
    "## Estrutura do modelo para 3 canais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec2cb5-faf4-48aa-9232-5a73d1084a52",
   "metadata": {},
   "source": [
    "![Descrição da imagem](model_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2aa105-ba63-40b1-9431-345e3575df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input = (None, 128, 64, 2)\n",
      "Conv block = (None, 128, 64, 64)\n",
      "Channel Attention = (None, 128, 64, 64)\n",
      "Channel Attention Last CNN = (None, 128, 64, 2)\n",
      "First phase = (None, 128, 64, 4)\n",
      "\n",
      "Multi-scale feature extraction = (None, 128, 64, 12)\n",
      "Kernel Selection Module = (None, 128, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "model = create_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8c73a2-3c38-48c5-a554-135483a317ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_layer (InputLayer)       [(None, 128, 64, 2)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " convolutional_block (Convoluti  (None, 128, 64, 64)  112000     ['input_layer[0][0]']            \n",
      " onal_block)                                                                                      \n",
      "                                                                                                  \n",
      " channel_attention (Channel_att  (None, 128, 64, 64)  322        ['convolutional_block[0][0]']    \n",
      " ention)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 64, 2)   1154        ['channel_attention[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128, 64, 4)   0           ['input_layer[0][0]',            \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " multi_scale_feature_extraction  (None, 128, 64, 12)  39073080   ['concatenate[0][0]']            \n",
      "  (Multi_scale_feature_extracti                                                                   \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " kernel_selecting_module (Kerne  (None, 128, 64, 21)  21212      ['multi_scale_feature_extraction[\n",
      " l_selecting_module)                                             0][0]']                          \n",
      "                                                                                                  \n",
      " conv2d_284 (Conv2D)            (None, 128, 64, 2)   380         ['kernel_selecting_module[0][0]']\n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128, 64, 2)   0           ['conv2d_284[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 39,208,148\n",
      "Trainable params: 39,204,292\n",
      "Non-trainable params: 3,856\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa6e46e-e1df-45b8-9522-68caa47aa640",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train = len(sound_base.train_X)\n",
    "steps_per_epoch_validation = len(sound_base.val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498460bc-1050-4060-89d8-0b5456c74f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_path = \"/tf/Etapa 3/Artigos/PRIDNet/model_checkpoints\"\n",
    "callbacks_lst = [\n",
    "                 tf.keras.callbacks.ModelCheckpoint(filepath=best_models_path+\"best_PRIDNet_blindnoise_128x64.h5\", save_freq=100, save_weights_only=False),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', min_lr=0.0000009, min_delta=0.0001, factor=0.70, patience=3, verbose=1, mode='min'),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, min_delta=0.0001, patience=10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85192611-4428-4dd7-8ef5-3a929017b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.MeanSquaredLogarithmicError(), optimizer=Adam(learning_rate=0.0009))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe747e8d-17f4-4639-8eee-f947a944f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1\n",
      " 744/1007 [=====================>........] - ETA: 2:16 - loss: 0.0250"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "steps_per_epoch = len(sound_base.train_X) // batch_size\n",
    "\n",
    "print('Starting training')\n",
    "\n",
    "for i, epoch in enumerate(range(20)):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    # Gera um novo lote de validação para cada época\n",
    "    validation_batch = next(data_generator_val.generate_sample_completo(batch_size=batch_size))\n",
    "    x_val, y_val = validation_batch\n",
    "\n",
    "    model.fit(data_generator_train.generate_sample_completo(batch_size=batch_size, include_clean=True),\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val),\n",
    "              # callbacks=callbacks_lst\n",
    "             )\n",
    "\n",
    "    indice_aleatorio = np.random.choice(x_val.shape[0])\n",
    "    \n",
    "    # Selecione a amostra correspondente\n",
    "    amostra_noisy_module = x_val[indice_aleatorio]\n",
    "    amostra_noisy_module = amostra_noisy_module[np.newaxis, ...]\n",
    "\n",
    "    amostra_original_module = y_val[indice_aleatorio]\n",
    "    amostra_original_module = amostra_original_module[np.newaxis, ...]\n",
    "    \n",
    "    generate_images(model, amostra_noisy_module, amostra_original_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c25d86-c5b8-464d-a568-ced15d2780da",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = next(data_generator_val.generate_sample_completo(batch_size=8))\n",
    "x_test, y_test = validation_batch\n",
    "\n",
    "x_test = x_test[0, ...]\n",
    "y_test = y_test[0, ...]\n",
    "\n",
    "x_test = x_test[np.newaxis, ...]\n",
    "y_test = y_test[np.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4264f-bd8e-4c1e-be83-1a3ef53f8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(x_test[:, :, : , 0]))\n",
    "print(np.min(x_test[:, :, : , 0]))\n",
    "print(np.max(y_test[:, :, : , 0]))\n",
    "print(np.min(y_test[:, :, : , 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9e9f5-50ab-4e03-a2dc-dd7c1cd76004",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_f = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72460ef5-ca1a-4085-ac90-4ed0dc15670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(F_f))\n",
    "print(np.min(F_f))\n",
    "print(F_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe73a5-f35e-4bfd-8834-cccecefe4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = y_test.reshape(y_test.shape[1], y_test.shape[2], y_test.shape[3])[:, :, 0]\n",
    "phi = (y_test.reshape(y_test.shape[1], y_test.shape[2], y_test.shape[3])[:, :, 1] - 0.5) * 2 * np.pi\n",
    "\n",
    "A_f = F_f.reshape(F_f.shape[1], F_f.shape[2], F_f.shape[3])[:, :, 0]\n",
    "phi_f = (F_f.reshape(F_f.shape[1], F_f.shape[2], F_f.shape[3])[:, :, 1] - 0.5) * 2 * np.pi\n",
    "\n",
    "A_n = x_test.reshape(x_test.shape[1], x_test.shape[2], x_test.shape[3])[:, :, 0]\n",
    "phi_n = (x_test.reshape(x_test.shape[1], x_test.shape[2], x_test.shape[3])[:, :, 1] - 0.5) * 2 * np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad77b55-5bde-4963-a578-497a46955c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(A_f))\n",
    "print(np.max(phi_f))\n",
    "print(np.min(A_f))\n",
    "print(np.min(phi_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa78ff-7929-4a19-aaff-42e41c4dc7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5503f-d37e-433b-b0da-274e40fce697",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = reconstruct_signal_from_stft(A, phi)\n",
    "s_n = reconstruct_signal_from_stft(A_n, phi_n)\n",
    "s_f = reconstruct_signal_from_stft(A_f, phi_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e3282-c2f6-4375-a957-22ead37810c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando figuras e eixos separados para cada array\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n",
    "\n",
    "# Plotando o primeiro array\n",
    "ax1.plot(s)\n",
    "ax1.set_ylabel('Sinal de voz ruidoso')\n",
    "\n",
    "# Plotando o primeiro array\n",
    "ax2.plot(s_n)\n",
    "ax2.set_ylabel('Sinal de voz ruidoso')\n",
    "\n",
    "ax3.plot(s_f)\n",
    "ax3.set_ylabel('Sinal de voz filtrado')\n",
    "\n",
    "# Exibindo os gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2425eb2-3e22-47ea-b657-bb2529d12a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=s, rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d397ab-a97a-46a6-a90f-85b4c61d22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=s_n, rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70d233-9c96-451b-a4bc-1d1d810e467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=s_f, rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacd80c-102e-4ac3-ac67-05be8b70128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892d85b-70ea-4924-ba77-7ea88a575475",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6359d7-4894-4354-82b0-ea0d5334552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "64/ (8*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e06c75-a521-43fa-a923-82b7f115b52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
